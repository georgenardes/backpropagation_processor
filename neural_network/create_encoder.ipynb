{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set bsize, num val images and list files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"acf_alv\":0, \"acf_chap\":1, \"reboco\":2, \"tijolo_alv\":3, \"tijolo_chap\":4}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_images_path = glob(\"resized_dataset_texturas_v2/*/*.jpeg\")\n",
    "random.shuffle(train_images_path)\n",
    "val_images_path = glob(\"resized_dataset_texturas_v2_test/*/*.jpeg\")\n",
    "random.shuffle(val_images_path)\n",
    "\n",
    "train_files = train_images_path\n",
    "val_files = val_images_path\n",
    "\n",
    "def filename_to_label(file_names, class_map_, class_index=1):\n",
    "    labels = []\n",
    "    for file_name in file_names:\n",
    "        labels.append(class_map_[file_name.split(\"/\")[class_index]])\n",
    "    return labels\n",
    "\n",
    "train_labels = filename_to_label(train_files, class_map)\n",
    "val_labels = filename_to_label(val_files, class_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define data handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image.set_shape([256, 256, 3])    \n",
    "    return image\n",
    "    \n",
    "\n",
    "def load_data_for_autoencoder(image_list):\n",
    "    image = read_image(image_list)    \n",
    "    image = tf.cast(image, tf.float16) / 256    \n",
    "    return image, image\n",
    "\n",
    "def load_data_for_classifier(image_list, label_list, oh_depth):\n",
    "    image = read_image(image_list)    \n",
    "    image = tf.cast(image, tf.float16) / 256    \n",
    "    label = tf.one_hot(label_list, oh_depth, 1.0, 0.0)\n",
    "    label.set_shape([5])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def autoencoder_data_generator(image_list, drop_remainder=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list))\n",
    "    dataset = dataset.shuffle(2048)        \n",
    "    dataset = dataset.map(load_data_for_autoencoder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=drop_remainder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    return dataset\n",
    "\n",
    "def classifier_data_generator(image_list, labels_list, oh_depth, drop_remainder=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list, labels_list, oh_depth))\n",
    "    dataset = dataset.shuffle(2048)        \n",
    "    dataset = dataset.map(load_data_for_classifier, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=drop_remainder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = autoencoder_data_generator(train_files)\n",
    "val_dataset = autoencoder_data_generator(val_files, False)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define autoencoder arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x = keras.layers.DepthwiseConv2D(\n",
    "        kernel_size=16,\n",
    "        strides=(16,16),\n",
    "        padding=\"same\",\n",
    "        depth_multiplier=8,\n",
    "    )(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        16,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\",\n",
    "    )(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(16, kernel_size=1, name=\"encoder_output\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder(x):\n",
    "    x = keras.layers.Conv2D(16, kernel_size=1, activation=\"tanh\", name=\"decoder_input\")(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\",\n",
    "    )(x)\n",
    "\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "    \n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\",\n",
    "    )(x)\n",
    "\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(\n",
    "        3, kernel_size=5, padding=\"same\", activation=\"sigmoid\", name=\"decoder_output\"\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def autoencoder():\n",
    "    input_layer = keras.layers.Input((256,256,3), name=\"input_layer\")    \n",
    "    encoder_output = encoder(input_layer)\n",
    "    decoder_output = decoder(encoder_output)\n",
    "\n",
    "    model = keras.Model(inputs=[input_layer], outputs=[decoder_output])\n",
    "    return model\n",
    "\n",
    "model = autoencoder()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(y_true, y_pred)))\n",
    "    # return tf.losses.binary_crossentropy(y_true, y_pred, label_smoothing=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_inicial = 0.00001\n",
    "lr_default = 0.001\n",
    "lr_final = 0.00001\n",
    "epochs = 100\n",
    "\n",
    "# This function warmup the learning rate in the first \n",
    "def schedulerWithWarmupAndDecay(epoch, lr):\n",
    "    if epoch < int(epochs * 0.1):\n",
    "        return lr + (lr_default / (epochs * 0.1 + 1))\n",
    "    elif epoch > int(epochs * 0.60):\n",
    "        return lr_final\n",
    "    else:\n",
    "        return lr_default\n",
    "\n",
    "# callback for updating the learning rate\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(schedulerWithWarmupAndDecay)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr_inicial)\n",
    "model.compile(optimizer=opt, loss=custom_loss)\n",
    "model.fit(train_dataset, epochs=epochs, callbacks=[lr_callback])\n",
    "\n",
    "# save\n",
    "model.save(\"saved_models/encoder_v3/\")\n",
    "model.save(\"saved_models/encoder_v3.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize inputs, outputs, and encoddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    r\"C:\\Users\\georg\\Desktop\\texture_encoder\\saved_models\\encoder_v2\",\n",
    "    custom_objects={\"custom_loss\": custom_loss},\n",
    ")\n",
    "\n",
    "debug_model = keras.Model(\n",
    "    inputs=[model.layers[0].input],\n",
    "    outputs=[model.layers[-1].output, model.get_layer(\"encoder_output\").output],\n",
    ")\n",
    "\n",
    "\n",
    "examples = train_dataset.take(1)\n",
    "for e in examples.as_numpy_iterator():\n",
    "    for img in e[0][0:10]:\n",
    "        output, encoder_output = debug_model.predict(img[tf.newaxis, ...])\n",
    "        encoder_output = np.mean(encoder_output, axis=-1, keepdims=True)\n",
    "\n",
    "        plt.imshow(img.astype(np.float32))\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(output[0].astype(np.float32))\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(encoder_output[0].astype(np.float32))\n",
    "        plt.show()\n",
    "\n",
    "        print(\"-------------------------\\n\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export encoddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in train_files + val_files:\n",
    "    img, lbl = load_data(image_path)\n",
    "    output, encoder_output = debug_model.predict(img[tf.newaxis, ...])\n",
    "\n",
    "    print(encoder_output.shape)\n",
    "    encoder_output = np.ravel(encoder_output)\n",
    "    \n",
    "    print(image_path)\n",
    "    np.save(image_path.replace(\".jpeg\", \"\"), encoder_output)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
