{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set bsize, num val images and list files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"acf_alv\":0, \"acf_chap\":1, \"reboco\":2, \"tijolo_alv\":3, \"tijolo_chap\":4}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_images_path = glob(\"resized_dataset_texturas_v2/*/*.jpeg\")\n",
    "random.shuffle(train_images_path)\n",
    "val_images_path = glob(\"resized_dataset_texturas_v2_test/*/*.jpeg\")\n",
    "random.shuffle(val_images_path)\n",
    "\n",
    "train_files = train_images_path\n",
    "val_files = val_images_path\n",
    "\n",
    "def filename_to_label(file_names, class_map_, class_index=1):\n",
    "    labels = []\n",
    "    for file_name in file_names:\n",
    "        labels.append(class_map_[file_name.split(\"/\")[class_index]])\n",
    "    return labels\n",
    "\n",
    "train_labels = filename_to_label(train_files, class_map)\n",
    "val_labels = filename_to_label(val_files, class_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define data handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image.set_shape([256, 256, 3])    \n",
    "    return image\n",
    "    \n",
    "\n",
    "def load_data_for_autoencoder(image_list):\n",
    "    image = read_image(image_list)    \n",
    "    image = tf.cast(image, tf.float16) / 256    \n",
    "    return image, image\n",
    "\n",
    "def load_data_for_classifier(image_list, label_list, oh_depth):\n",
    "    image = read_image(image_list)    \n",
    "    image = tf.cast(image, tf.float16) / 256    \n",
    "    label = tf.one_hot(label_list, oh_depth, 1.0, 0.0)\n",
    "    label.set_shape([5])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def autoencoder_data_generator(image_list, drop_remainder=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list))\n",
    "    dataset = dataset.shuffle(2048)        \n",
    "    dataset = dataset.map(load_data_for_autoencoder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=drop_remainder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    return dataset\n",
    "\n",
    "def classifier_data_generator(image_list, labels_list, oh_depth, drop_remainder=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list, labels_list, oh_depth))\n",
    "    dataset = dataset.shuffle(2048)        \n",
    "    dataset = dataset.map(load_data_for_classifier, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=drop_remainder, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = autoencoder_data_generator(train_files)\n",
    "val_dataset = autoencoder_data_generator(val_files, False)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model = tf.keras.models.load_model(\n",
    "    r\"./saved_models/encoder_v3\",\n",
    "    custom_objects={\"custom_loss\": None},\n",
    ")\n",
    "\n",
    "autoencoder_model.trainable = False\n",
    "model_input = autoencoder_model.layers[0].input\n",
    "encoder_output = autoencoder_model.get_layer(\"encoder_output\").output\n",
    "encoder = keras.Model(inputs=[model_input], outputs=[encoder_output])\n",
    "\n",
    "x = x_in = keras.layers.Input((256,256,3))\n",
    "\n",
    "encoder_output = encoder(x)\n",
    "\n",
    "\n",
    "x = keras.layers.Flatten()(encoder_output)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "x = keras.layers.Dense(5, \"softmax\")(x)\n",
    "\n",
    "# create model\n",
    "classifier = keras.Model(inputs=[x_in], outputs=[x])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load classifier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = classifier_data_generator(\n",
    "    train_files, train_labels, oh_depth=tf.constant([5] * len(train_labels))\n",
    ")\n",
    "val_dataset = classifier_data_generator(\n",
    "    val_files,\n",
    "    val_labels,\n",
    "    oh_depth=tf.constant([5] * len(val_labels)),\n",
    "    drop_remainder=False,\n",
    ")\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_inicial = 0.00001\n",
    "lr_default = 0.001\n",
    "lr_final = 0.00001\n",
    "epochs = 100\n",
    "\n",
    "# This function warmup the learning rate in the first \n",
    "def schedulerWithWarmupAndDecay(epoch, lr):\n",
    "    if epoch < int(epochs * 0.1):\n",
    "        return lr + (lr_default / (epochs * 0.1 + 1))\n",
    "    elif epoch > int(epochs * 0.60):\n",
    "        return lr_final\n",
    "    else:\n",
    "        return lr_default\n",
    "\n",
    "# callback for updating the learning rate\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(schedulerWithWarmupAndDecay)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr_inicial)\n",
    "classifier.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])\n",
    "history = classifier.fit(train_dataset, epochs=epochs, callbacks=[lr_callback])\n",
    "\n",
    "# save classifier\n",
    "classifier.save(\"saved_models/classifier_v2/\", overwrite=False)\n",
    "classifier.save(\"saved_models/classifier_v2.h5\", overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.models.load_model(\n",
    "    r\"./saved_models/classifier_v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_name_map = [\"acf_alv\", \"acf_chap\", \"reboco\", \"tijolo_alv\", \"tijolo_chap\"]\n",
    "\n",
    "examples = val_dataset.take(1)\n",
    "for e in examples.as_numpy_iterator():\n",
    "    imgs = e[0]\n",
    "    lbls = e[1]\n",
    "    for img, lbl in zip(imgs, lbls):\n",
    "        print(lbl.shape)\n",
    "        prediction = classifier.predict(img[tf.newaxis, ...])[0]\n",
    "        classifier_prediciton = class_name_map[np.argmax(prediction)]\n",
    "        \n",
    "        plt.imshow(img.astype(np.float32))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        print(classifier_prediciton)\n",
    "\n",
    "        print(\"-------------------------\\n\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "class_name_map = [\"acf_alv\", \"acf_chap\", \"reboco\", \"tijolo_alv\", \"tijolo_chap\"]\n",
    "\n",
    "save_path = \"classifier_predictions/\"\n",
    "\n",
    "examples = train_dataset.take(-1)\n",
    "i = 0\n",
    "for e in examples.as_numpy_iterator():\n",
    "    imgs = e[0]\n",
    "    lbls = e[1]\n",
    "    for img, lbl in zip(imgs, lbls):\n",
    "        print(lbl.shape)\n",
    "        prediction = classifier.predict(img[tf.newaxis, ...])[0]\n",
    "        classifier_prediciton = class_name_map[np.argmax(prediction)]\n",
    "        write_image_path = save_path + classifier_prediciton\n",
    "\n",
    "        # plt.imshow(img.astype(np.float32))\n",
    "        # plt.show()\n",
    "\n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists(write_image_path):\n",
    "            os.makedirs(write_image_path)\n",
    "        \n",
    "        img = (img*255).astype(np.uint8)        \n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        cv.imwrite(write_image_path + f\"/{class_name_map[np.argmax(lbl)]}_{i}.jpeg\", img)\n",
    "\n",
    "        print(classifier_prediciton)\n",
    "        print(\"-------------------------\\n\\n\")\n",
    "        i += 1\n",
    "\n",
    "\n",
    "examples = val_dataset.take(-1)\n",
    "for e in examples.as_numpy_iterator():\n",
    "    imgs = e[0]\n",
    "    lbls = e[1]\n",
    "    for img, lbl in zip(imgs, lbls):\n",
    "        print(lbl.shape)\n",
    "        prediction = classifier.predict(img[tf.newaxis, ...])[0]\n",
    "        classifier_prediciton = class_name_map[np.argmax(prediction)]\n",
    "        write_image_path = save_path + classifier_prediciton\n",
    "\n",
    "        # plt.imshow(img.astype(np.float32))\n",
    "        # plt.show()\n",
    "\n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists(write_image_path):\n",
    "            os.makedirs(write_image_path)\n",
    "        \n",
    "        img = (img*255).astype(np.uint8)        \n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        cv.imwrite(write_image_path + f\"/{class_name_map[np.argmax(lbl)]}_{i}.jpeg\", img)\n",
    "\n",
    "        print(classifier_prediciton)\n",
    "        print(\"-------------------------\\n\\n\")\n",
    "        i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract true labels from the dataset\n",
    "true_labels = []\n",
    "imgs = []\n",
    "for _, label in val_dataset:\n",
    "    imgs.append(_)\n",
    "    true_labels.append(label.numpy())\n",
    "\n",
    "true_labels = np.concatenate(true_labels)\n",
    "true_labels = np.argmax(true_labels, axis=-1)\n",
    "print(true_labels)\n",
    "\n",
    "predicted_labels = classifier.predict(imgs)\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=class_name_map)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
