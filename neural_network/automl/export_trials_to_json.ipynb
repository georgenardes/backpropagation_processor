{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export trials to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/webphy/Desktop/dnn_processor/\")  # just to enable `dataset`\n",
    "sys.path.append(\n",
    "    \"/home/webphy/Desktop/dnn_processor/dataset/\"\n",
    ")  # just to enable `dataset`\n",
    "import dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# load dataset\n",
    "train_images_rgx = \"../../dataset/resized_dataset_texturas_v2/*/*.jpeg\"\n",
    "valid_images_rgx = \"../../dataset/resized_dataset_texturas_v2_test/*/*.jpeg\"\n",
    "train_ds, val_ds, nclass = dataset.create_datasets(\n",
    "    train_images_rgx, valid_images_rgx, 64\n",
    ")\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for layer in model.trainable_variables:\n",
    "        layer_params = tf.reduce_prod(layer.shape)\n",
    "        total_params += layer_params\n",
    "    return total_params.numpy()\n",
    "\n",
    "\n",
    "def count_parameters_of_layer(layer):\n",
    "    total_params = 0\n",
    "    for l in layer.trainable_variables:\n",
    "        layer_params = tf.reduce_prod(l.shape)\n",
    "        total_params += layer_params\n",
    "    return total_params.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = \"exp4_with_aug\"\n",
    "\n",
    "saved_models_root = f\"saved_models/{exp_id}/*/\"\n",
    "models_path = glob(saved_models_root)\n",
    "assert len(models_path) > 0, \"nenhum arquivo listado\"\n",
    "\n",
    "trials = {}\n",
    "keras.utils.disable_interactive_logging()\n",
    "for p in models_path:\n",
    "    print(p)\n",
    "    trial_id = p.split(\"model_t\")[1][:-1]\n",
    "\n",
    "    # load model and weights\n",
    "    model = keras.models.load_model(p)\n",
    "    model.summary()\n",
    "    model.load_weights(p + \"/weights/\")\n",
    "\n",
    "    # evaluate\n",
    "    val_score = model.evaluate(val_ds, verbose=1)\n",
    "\n",
    "    # if the val acc is less than 90% the model is discarded\n",
    "    if val_score[1] < 0.9:\n",
    "        continue\n",
    "\n",
    "    train_score = model.evaluate(train_ds, verbose=1)\n",
    "\n",
    "    model_num_params = count_parameters(model)\n",
    "    dense_num_params = count_parameters_of_layer(model.layers[-1])\n",
    "\n",
    "    trials[int(trial_id)] = {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"train_loss\": train_score[0],\n",
    "        \"train_accuracy\": train_score[1],\n",
    "        \"val_loss\": val_score[0],\n",
    "        \"val_accuracy\": val_score[1],\n",
    "        \"num_params\": model_num_params,\n",
    "        \"dense_num_params\": dense_num_params,\n",
    "        \"model_config\": model.get_config(),\n",
    "    }\n",
    "\n",
    "\n",
    "with open(f\"{exp_id}.json\", \"w\") as f:\n",
    "    json.dump(trials, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples with (num_params, object)\n",
    "ordered_list = [(trials[key][\"num_params\"], trials[key]) for key in trials]\n",
    "\n",
    "# Sort the list based on num_params\n",
    "ordered_list.sort(key=lambda x: x[0])\n",
    "\n",
    "# Extract the objects from the sorted list\n",
    "ordered_objects = [obj for _, obj in ordered_list]\n",
    "\n",
    "with open(f\"{exp_id}_ordered_objects.json\", \"w\") as f:\n",
    "    json.dump(ordered_objects, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
